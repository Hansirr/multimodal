{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SensorDataset import SensorDataset\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 145, 19)\n"
     ]
    }
   ],
   "source": [
    "sensor_dataset = SensorDataset(\"/home/rafaelpossas/dev/multimodal_dataset/sensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_count = len(sensor_dataset.x_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(sensor_dataset.x_test)  # 2947 testing series\n",
    "n_steps = len(sensor_dataset.x_train[0])  # 128 timesteps per series\n",
    "n_input = len(sensor_dataset.x_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 16 # Hidden layer num of features\n",
    "n_classes = 20 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "batch_size = 20\n",
    "display_iter = 100  # To show test set accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset: \n",
    "    # https://tensorhub.com/aymericdamien/tensorflow-rnn\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.nn.rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20:   Batch Loss = 4.240007, Accuracy = 0.10000000149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.72674274445, Accuracy = 0.0151515156031\n",
      "Training iter #100:   Batch Loss = 4.073685, Accuracy = 0.0500000007451\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.52675199509, Accuracy = 0.0151515156031\n",
      "Training iter #200:   Batch Loss = 3.729147, Accuracy = 0.10000000149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.48767852783, Accuracy = 0.0303030312061\n",
      "Training iter #300:   Batch Loss = 3.433275, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.41149568558, Accuracy = 0.0454545468092\n",
      "Training iter #400:   Batch Loss = 3.851238, Accuracy = 0.0500000007451\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.38499069214, Accuracy = 0.0454545468092\n",
      "Training iter #500:   Batch Loss = 3.596219, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.37555837631, Accuracy = 0.0454545468092\n",
      "Training iter #600:   Batch Loss = 3.249743, Accuracy = 0.10000000149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.37423181534, Accuracy = 0.0303030312061\n",
      "Training iter #700:   Batch Loss = 3.074957, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.3953127861, Accuracy = 0.0151515156031\n",
      "Training iter #800:   Batch Loss = 3.510264, Accuracy = 0.10000000149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.35005235672, Accuracy = 0.0303030312061\n",
      "Training iter #900:   Batch Loss = 3.159326, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.3555059433, Accuracy = 0.0303030312061\n",
      "Training iter #1000:   Batch Loss = 3.022596, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.39387750626, Accuracy = 0.0606060624123\n",
      "Training iter #1100:   Batch Loss = 2.689012, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.42073583603, Accuracy = 0.0757575780153\n",
      "Training iter #1200:   Batch Loss = 3.307240, Accuracy = 0.10000000149\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.42880725861, Accuracy = 0.0757575780153\n",
      "Training iter #1300:   Batch Loss = 2.916905, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.40341234207, Accuracy = 0.0909090936184\n",
      "Training iter #1400:   Batch Loss = 2.678155, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.4373588562, Accuracy = 0.121212124825\n",
      "Training iter #1500:   Batch Loss = 2.542004, Accuracy = 0.350000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.4543967247, Accuracy = 0.106060609221\n",
      "Training iter #1600:   Batch Loss = 2.964163, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.40205383301, Accuracy = 0.106060609221\n",
      "Training iter #1700:   Batch Loss = 2.677431, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.44422197342, Accuracy = 0.0909090936184\n",
      "Training iter #1800:   Batch Loss = 2.554536, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.4572687149, Accuracy = 0.0909090936184\n",
      "Training iter #1900:   Batch Loss = 2.457212, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.53738307953, Accuracy = 0.0909090936184\n",
      "Training iter #2000:   Batch Loss = 2.754388, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.58114242554, Accuracy = 0.0757575780153\n",
      "Training iter #2100:   Batch Loss = 2.243970, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.70265245438, Accuracy = 0.0606060624123\n",
      "Training iter #2200:   Batch Loss = 2.349719, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.72659397125, Accuracy = 0.0454545468092\n",
      "Training iter #2300:   Batch Loss = 2.276456, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7505698204, Accuracy = 0.0454545468092\n",
      "Training iter #2400:   Batch Loss = 2.483910, Accuracy = 0.450000017881\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.71505117416, Accuracy = 0.0606060624123\n",
      "Training iter #2500:   Batch Loss = 2.109878, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7517323494, Accuracy = 0.0909090936184\n",
      "Training iter #2600:   Batch Loss = 2.400671, Accuracy = 0.450000017881\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.70749950409, Accuracy = 0.0909090936184\n",
      "Training iter #2700:   Batch Loss = 1.997841, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.78659534454, Accuracy = 0.0757575780153\n",
      "Training iter #2800:   Batch Loss = 2.097503, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.74269723892, Accuracy = 0.0757575780153\n",
      "Training iter #2900:   Batch Loss = 2.225098, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.65581226349, Accuracy = 0.0606060624123\n",
      "Training iter #3000:   Batch Loss = 2.312365, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.71718645096, Accuracy = 0.0757575780153\n",
      "Training iter #3100:   Batch Loss = 2.312757, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7996544838, Accuracy = 0.0606060624123\n",
      "Training iter #3200:   Batch Loss = 2.151806, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.76808166504, Accuracy = 0.0606060624123\n",
      "Training iter #3300:   Batch Loss = 2.030104, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.72493171692, Accuracy = 0.0909090936184\n",
      "Training iter #3400:   Batch Loss = 2.257232, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.6989531517, Accuracy = 0.0757575780153\n",
      "Training iter #3500:   Batch Loss = 1.912680, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.65141534805, Accuracy = 0.0454545468092\n",
      "Training iter #3600:   Batch Loss = 2.254269, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.78184461594, Accuracy = 0.0606060624123\n",
      "Training iter #3700:   Batch Loss = 1.869497, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.80516052246, Accuracy = 0.0454545468092\n",
      "Training iter #3800:   Batch Loss = 1.988129, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.79559230804, Accuracy = 0.0454545468092\n",
      "Training iter #3900:   Batch Loss = 1.974782, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.83761739731, Accuracy = 0.0454545468092\n",
      "Training iter #4000:   Batch Loss = 1.806353, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7850484848, Accuracy = 0.0757575780153\n",
      "Training iter #4100:   Batch Loss = 1.835288, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.86223745346, Accuracy = 0.0757575780153\n",
      "Training iter #4200:   Batch Loss = 2.066307, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.58776140213, Accuracy = 0.0909090936184\n",
      "Training iter #4300:   Batch Loss = 1.895899, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.67340040207, Accuracy = 0.0606060624123\n",
      "Training iter #4400:   Batch Loss = 1.899122, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.79269886017, Accuracy = 0.0454545468092\n",
      "Training iter #4500:   Batch Loss = 1.737970, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.70645618439, Accuracy = 0.0606060624123\n",
      "Training iter #4600:   Batch Loss = 2.205696, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7147769928, Accuracy = 0.0606060624123\n",
      "Training iter #4700:   Batch Loss = 1.951893, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.7152299881, Accuracy = 0.0757575780153\n",
      "Training iter #4800:   Batch Loss = 1.983148, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.74687767029, Accuracy = 0.0757575780153\n",
      "Training iter #4900:   Batch Loss = 1.591092, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.73841810226, Accuracy = 0.106060609221\n",
      "Training iter #5000:   Batch Loss = 1.899101, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.71507024765, Accuracy = 0.106060609221\n",
      "Training iter #5100:   Batch Loss = 2.009723, Accuracy = 0.450000017881\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.80822563171, Accuracy = 0.0606060624123\n",
      "Training iter #5200:   Batch Loss = 2.010716, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.86453390121, Accuracy = 0.0606060624123\n",
      "Training iter #5300:   Batch Loss = 1.453586, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.86270999908, Accuracy = 0.0909090936184\n",
      "Training iter #5400:   Batch Loss = 1.720897, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.88705778122, Accuracy = 0.0757575780153\n",
      "Training iter #5500:   Batch Loss = 2.083601, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.94101285934, Accuracy = 0.0909090936184\n",
      "Training iter #5600:   Batch Loss = 1.880167, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.87146759033, Accuracy = 0.0909090936184\n",
      "Training iter #5700:   Batch Loss = 1.929711, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.8479385376, Accuracy = 0.106060609221\n",
      "Training iter #5800:   Batch Loss = 2.107109, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.86184883118, Accuracy = 0.106060609221\n",
      "Training iter #5900:   Batch Loss = 2.033599, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.77049827576, Accuracy = 0.121212124825\n",
      "Training iter #6000:   Batch Loss = 1.693246, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.75241565704, Accuracy = 0.106060609221\n",
      "Training iter #6100:   Batch Loss = 1.667617, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.81898021698, Accuracy = 0.106060609221\n",
      "Training iter #6200:   Batch Loss = 1.614901, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.84735250473, Accuracy = 0.106060609221\n",
      "Training iter #6300:   Batch Loss = 1.795536, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.89463806152, Accuracy = 0.106060609221\n",
      "Training iter #6400:   Batch Loss = 1.634535, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.82502555847, Accuracy = 0.106060609221\n",
      "Training iter #6500:   Batch Loss = 1.496069, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.89826107025, Accuracy = 0.0909090936184\n",
      "Training iter #6600:   Batch Loss = 1.718580, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.91583633423, Accuracy = 0.0909090936184\n",
      "Training iter #6700:   Batch Loss = 1.604951, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.91926574707, Accuracy = 0.0909090936184\n",
      "Training iter #6800:   Batch Loss = 1.433169, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.95701217651, Accuracy = 0.106060609221\n",
      "Training iter #6900:   Batch Loss = 1.424336, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.91743326187, Accuracy = 0.0909090936184\n",
      "Training iter #7000:   Batch Loss = 1.592844, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.946808815, Accuracy = 0.0909090936184\n",
      "Training iter #7100:   Batch Loss = 1.447039, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.975233078, Accuracy = 0.106060609221\n",
      "Training iter #7200:   Batch Loss = 1.336163, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.03115797043, Accuracy = 0.106060609221\n",
      "Training iter #7300:   Batch Loss = 1.190027, Accuracy = 0.899999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.04854154587, Accuracy = 0.0909090936184\n",
      "Training iter #7400:   Batch Loss = 1.575765, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.10809326172, Accuracy = 0.0909090936184\n",
      "Training iter #7500:   Batch Loss = 1.423032, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0920419693, Accuracy = 0.0909090936184\n",
      "Training iter #7600:   Batch Loss = 1.294702, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08153009415, Accuracy = 0.0909090936184\n",
      "Training iter #7700:   Batch Loss = 1.315721, Accuracy = 0.899999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.12644815445, Accuracy = 0.0757575780153\n",
      "Training iter #7800:   Batch Loss = 1.553671, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.12440395355, Accuracy = 0.106060609221\n",
      "Training iter #7900:   Batch Loss = 1.333805, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0858464241, Accuracy = 0.0909090936184\n",
      "Training iter #8000:   Batch Loss = 1.357940, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18931102753, Accuracy = 0.0757575780153\n",
      "Training iter #8100:   Batch Loss = 1.430164, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.41142988205, Accuracy = 0.0454545468092\n",
      "Training iter #8200:   Batch Loss = 1.552459, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.33219528198, Accuracy = 0.0606060624123\n",
      "Training iter #8300:   Batch Loss = 2.672048, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.39271020889, Accuracy = 0.0909090936184\n",
      "Training iter #8400:   Batch Loss = 2.601992, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.36047458649, Accuracy = 0.0606060624123\n",
      "Training iter #8500:   Batch Loss = 2.549894, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.33308839798, Accuracy = 0.0757575780153\n",
      "Training iter #8600:   Batch Loss = 2.069202, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.37134075165, Accuracy = 0.0909090936184\n",
      "Training iter #8700:   Batch Loss = 3.056015, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.21736431122, Accuracy = 0.0909090936184\n",
      "Training iter #8800:   Batch Loss = 2.247076, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.06361484528, Accuracy = 0.0606060624123\n",
      "Training iter #8900:   Batch Loss = 2.625080, Accuracy = 0.450000017881\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.04662656784, Accuracy = 0.0606060624123\n",
      "Training iter #9000:   Batch Loss = 1.861755, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.09506750107, Accuracy = 0.0606060624123\n",
      "Training iter #9100:   Batch Loss = 2.505512, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.04295444489, Accuracy = 0.0303030312061\n",
      "Training iter #9200:   Batch Loss = 2.194704, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.11630105972, Accuracy = 0.0606060624123\n",
      "Training iter #9300:   Batch Loss = 2.955725, Accuracy = 0.34999999404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.03874444962, Accuracy = 0.0606060624123\n",
      "Training iter #9400:   Batch Loss = 1.951428, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.19112825394, Accuracy = 0.0606060624123\n",
      "Training iter #9500:   Batch Loss = 2.510794, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.28372144699, Accuracy = 0.0606060624123\n",
      "Training iter #9600:   Batch Loss = 1.994026, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.30165958405, Accuracy = 0.0757575780153\n",
      "Training iter #9700:   Batch Loss = 2.789152, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.24578380585, Accuracy = 0.0606060624123\n",
      "Training iter #9800:   Batch Loss = 1.542255, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.21623945236, Accuracy = 0.0606060624123\n",
      "Training iter #9900:   Batch Loss = 2.384298, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.23902702332, Accuracy = 0.0606060624123\n",
      "Training iter #10000:   Batch Loss = 2.030498, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.21054601669, Accuracy = 0.0606060624123\n",
      "Training iter #10100:   Batch Loss = 2.259414, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.11210775375, Accuracy = 0.0454545468092\n",
      "Training iter #10200:   Batch Loss = 1.341419, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14060163498, Accuracy = 0.0606060624123\n",
      "Training iter #10300:   Batch Loss = 2.434935, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17503833771, Accuracy = 0.0757575780153\n",
      "Training iter #10400:   Batch Loss = 1.856263, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20847654343, Accuracy = 0.0909090936184\n",
      "Training iter #10500:   Batch Loss = 1.843586, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.31563472748, Accuracy = 0.0606060624123\n",
      "Training iter #10600:   Batch Loss = 1.305752, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.49356079102, Accuracy = 0.0606060624123\n",
      "Training iter #10700:   Batch Loss = 1.886476, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.45602703094, Accuracy = 0.0303030312061\n",
      "Training iter #10800:   Batch Loss = 1.846128, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.46219396591, Accuracy = 0.0303030312061\n",
      "Training iter #10900:   Batch Loss = 2.151040, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.52972459793, Accuracy = 0.0303030312061\n",
      "Training iter #11000:   Batch Loss = 1.725541, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.55897283554, Accuracy = 0.0606060624123\n",
      "Training iter #11100:   Batch Loss = 1.632195, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.57953214645, Accuracy = 0.0606060624123\n",
      "Training iter #11200:   Batch Loss = 1.784630, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.45775175095, Accuracy = 0.0909090936184\n",
      "Training iter #11300:   Batch Loss = 1.947813, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.33120584488, Accuracy = 0.0606060624123\n",
      "Training iter #11400:   Batch Loss = 1.792830, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.35142993927, Accuracy = 0.0606060624123\n",
      "Training iter #11500:   Batch Loss = 1.583039, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.33931589127, Accuracy = 0.0606060624123\n",
      "Training iter #11600:   Batch Loss = 1.701111, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.35667419434, Accuracy = 0.0606060624123\n",
      "Training iter #11700:   Batch Loss = 1.594985, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.35066604614, Accuracy = 0.0757575780153\n",
      "Training iter #11800:   Batch Loss = 1.887719, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.31675291061, Accuracy = 0.0606060624123\n",
      "Training iter #11900:   Batch Loss = 1.500967, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.34914779663, Accuracy = 0.0606060624123\n",
      "Training iter #12000:   Batch Loss = 1.754009, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.35965204239, Accuracy = 0.0909090936184\n",
      "Training iter #12100:   Batch Loss = 1.758411, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18308401108, Accuracy = 0.0757575780153\n",
      "Training iter #12200:   Batch Loss = 1.701328, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14935350418, Accuracy = 0.0757575780153\n",
      "Training iter #12300:   Batch Loss = 1.770777, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.07323598862, Accuracy = 0.0757575780153\n",
      "Training iter #12400:   Batch Loss = 1.502846, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15309524536, Accuracy = 0.0757575780153\n",
      "Training iter #12500:   Batch Loss = 1.772375, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.16715431213, Accuracy = 0.0909090936184\n",
      "Training iter #12600:   Batch Loss = 1.919533, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17168235779, Accuracy = 0.0909090936184\n",
      "Training iter #12700:   Batch Loss = 1.535768, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15453910828, Accuracy = 0.0909090936184\n",
      "Training iter #12800:   Batch Loss = 1.522527, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17687368393, Accuracy = 0.0757575780153\n",
      "Training iter #12900:   Batch Loss = 1.476280, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.19826364517, Accuracy = 0.0909090936184\n",
      "Training iter #13000:   Batch Loss = 1.720674, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.22234725952, Accuracy = 0.0909090936184\n",
      "Training iter #13100:   Batch Loss = 1.472221, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20525407791, Accuracy = 0.0757575780153\n",
      "Training iter #13200:   Batch Loss = 1.627240, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15769815445, Accuracy = 0.0909090936184\n",
      "Training iter #13300:   Batch Loss = 1.400958, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18203926086, Accuracy = 0.0757575780153\n",
      "Training iter #13400:   Batch Loss = 2.257613, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20444965363, Accuracy = 0.0757575780153\n",
      "Training iter #13500:   Batch Loss = 1.584256, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.24123764038, Accuracy = 0.0757575780153\n",
      "Training iter #13600:   Batch Loss = 1.522262, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.2564997673, Accuracy = 0.0909090936184\n",
      "Training iter #13700:   Batch Loss = 1.270931, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.27765417099, Accuracy = 0.0757575780153\n",
      "Training iter #13800:   Batch Loss = 1.782606, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.33543157578, Accuracy = 0.106060609221\n",
      "Training iter #13900:   Batch Loss = 1.749019, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.35661649704, Accuracy = 0.136363640428\n",
      "Training iter #14000:   Batch Loss = 1.282187, Accuracy = 0.899999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.56524324417, Accuracy = 0.0909090936184\n",
      "Training iter #14100:   Batch Loss = 1.453471, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.44034290314, Accuracy = 0.0606060624123\n",
      "Training iter #14200:   Batch Loss = 2.378709, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.60371732712, Accuracy = 0.0606060624123\n",
      "Training iter #14300:   Batch Loss = 1.937080, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.55324888229, Accuracy = 0.0606060624123\n",
      "Training iter #14400:   Batch Loss = 2.397717, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.51059007645, Accuracy = 0.0757575780153\n",
      "Training iter #14500:   Batch Loss = 2.556740, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.40566158295, Accuracy = 0.0909090936184\n",
      "Training iter #14600:   Batch Loss = 2.333681, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.50367069244, Accuracy = 0.0909090936184\n",
      "Training iter #14700:   Batch Loss = 2.088998, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.3906083107, Accuracy = 0.0757575780153\n",
      "Training iter #14800:   Batch Loss = 2.601061, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.26624155045, Accuracy = 0.0606060624123\n",
      "Training iter #14900:   Batch Loss = 2.434335, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08826208115, Accuracy = 0.0757575780153\n",
      "Training iter #15000:   Batch Loss = 2.136440, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.1352853775, Accuracy = 0.0454545468092\n",
      "Training iter #15100:   Batch Loss = 1.923658, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.26316642761, Accuracy = 0.0303030312061\n",
      "Training iter #15200:   Batch Loss = 2.667145, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.2345328331, Accuracy = 0.0454545468092\n",
      "Training iter #15300:   Batch Loss = 2.206397, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.09464502335, Accuracy = 0.0303030312061\n",
      "Training iter #15400:   Batch Loss = 2.526623, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0513753891, Accuracy = 0.0303030312061\n",
      "Training iter #15500:   Batch Loss = 1.809480, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.03003740311, Accuracy = 0.0757575780153\n",
      "Training iter #15600:   Batch Loss = 2.359125, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.99387359619, Accuracy = 0.0757575780153\n",
      "Training iter #15700:   Batch Loss = 2.286089, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.91142177582, Accuracy = 0.0757575780153\n",
      "Training iter #15800:   Batch Loss = 1.733533, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.92063760757, Accuracy = 0.0757575780153\n",
      "Training iter #15900:   Batch Loss = 1.554785, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.92439222336, Accuracy = 0.0757575780153\n",
      "Training iter #16000:   Batch Loss = 2.042965, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.91068553925, Accuracy = 0.0757575780153\n",
      "Training iter #16100:   Batch Loss = 2.150576, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.00955867767, Accuracy = 0.0757575780153\n",
      "Training iter #16200:   Batch Loss = 1.618227, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08078956604, Accuracy = 0.0606060624123\n",
      "Training iter #16300:   Batch Loss = 1.523605, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.10543823242, Accuracy = 0.0606060624123\n",
      "Training iter #16400:   Batch Loss = 1.931254, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08955764771, Accuracy = 0.0606060624123\n",
      "Training iter #16500:   Batch Loss = 2.064131, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.10610151291, Accuracy = 0.0606060624123\n",
      "Training iter #16600:   Batch Loss = 1.576356, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08286571503, Accuracy = 0.0454545468092\n",
      "Training iter #16700:   Batch Loss = 1.719463, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.03777885437, Accuracy = 0.0454545468092\n",
      "Training iter #16800:   Batch Loss = 1.423060, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.05084991455, Accuracy = 0.0454545468092\n",
      "Training iter #16900:   Batch Loss = 1.532361, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.13733148575, Accuracy = 0.0454545468092\n",
      "Training iter #17000:   Batch Loss = 1.742153, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.25039196014, Accuracy = 0.0606060624123\n",
      "Training iter #17100:   Batch Loss = 1.585140, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.13601493835, Accuracy = 0.0606060624123\n",
      "Training iter #17200:   Batch Loss = 1.540557, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14187335968, Accuracy = 0.0606060624123\n",
      "Training iter #17300:   Batch Loss = 1.450422, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.11363124847, Accuracy = 0.0606060624123\n",
      "Training iter #17400:   Batch Loss = 1.446784, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.12121009827, Accuracy = 0.0606060624123\n",
      "Training iter #17500:   Batch Loss = 1.411384, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15924596786, Accuracy = 0.0606060624123\n",
      "Training iter #17600:   Batch Loss = 1.532467, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15799045563, Accuracy = 0.0303030312061\n",
      "Training iter #17700:   Batch Loss = 1.306948, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15259647369, Accuracy = 0.0454545468092\n",
      "Training iter #17800:   Batch Loss = 1.243770, Accuracy = 0.899999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20336294174, Accuracy = 0.0303030312061\n",
      "Training iter #17900:   Batch Loss = 1.465907, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.19608879089, Accuracy = 0.0454545468092\n",
      "Training iter #18000:   Batch Loss = 1.539966, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.22092866898, Accuracy = 0.0454545468092\n",
      "Training iter #18100:   Batch Loss = 1.142299, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.28164672852, Accuracy = 0.0454545468092\n",
      "Training iter #18200:   Batch Loss = 1.323566, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.23665380478, Accuracy = 0.0454545468092\n",
      "Training iter #18300:   Batch Loss = 1.534512, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.24110651016, Accuracy = 0.0454545468092\n",
      "Training iter #18400:   Batch Loss = 1.526376, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.30174589157, Accuracy = 0.0606060624123\n",
      "Training iter #18500:   Batch Loss = 1.762578, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.26985454559, Accuracy = 0.0454545468092\n",
      "Training iter #18600:   Batch Loss = 1.875869, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.22862529755, Accuracy = 0.0454545468092\n",
      "Training iter #18700:   Batch Loss = 1.870835, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.19291400909, Accuracy = 0.0606060624123\n",
      "Training iter #18800:   Batch Loss = 2.049199, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.042075634, Accuracy = 0.0606060624123\n",
      "Training iter #18900:   Batch Loss = 1.785411, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.97960805893, Accuracy = 0.0606060624123\n",
      "Training iter #19000:   Batch Loss = 1.577798, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.00591087341, Accuracy = 0.0606060624123\n",
      "Training iter #19100:   Batch Loss = 1.607295, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.94153642654, Accuracy = 0.0757575780153\n",
      "Training iter #19200:   Batch Loss = 2.229494, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.10370397568, Accuracy = 0.0757575780153\n",
      "Training iter #19300:   Batch Loss = 1.699035, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.01412725449, Accuracy = 0.0757575780153\n",
      "Training iter #19400:   Batch Loss = 1.500037, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.00457859039, Accuracy = 0.0757575780153\n",
      "Training iter #19500:   Batch Loss = 1.748140, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.07634735107, Accuracy = 0.0606060624123\n",
      "Training iter #19600:   Batch Loss = 1.651842, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.08659648895, Accuracy = 0.0606060624123\n",
      "Training iter #19700:   Batch Loss = 1.705565, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.07662343979, Accuracy = 0.0606060624123\n",
      "Training iter #19800:   Batch Loss = 1.362679, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.09201860428, Accuracy = 0.0606060624123\n",
      "Training iter #19900:   Batch Loss = 1.596759, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.09631347656, Accuracy = 0.0606060624123\n",
      "Training iter #20000:   Batch Loss = 1.895139, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14105939865, Accuracy = 0.0606060624123\n",
      "Training iter #20100:   Batch Loss = 1.960507, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.11793041229, Accuracy = 0.0606060624123\n",
      "Training iter #20200:   Batch Loss = 1.447431, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14759111404, Accuracy = 0.0606060624123\n",
      "Training iter #20300:   Batch Loss = 1.799151, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.1144618988, Accuracy = 0.0606060624123\n",
      "Training iter #20400:   Batch Loss = 1.766242, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.05024337769, Accuracy = 0.0606060624123\n",
      "Training iter #20500:   Batch Loss = 1.958252, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0335817337, Accuracy = 0.0606060624123\n",
      "Training iter #20600:   Batch Loss = 1.457887, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.06579780579, Accuracy = 0.0606060624123\n",
      "Training iter #20700:   Batch Loss = 1.407940, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.05622434616, Accuracy = 0.0606060624123\n",
      "Training iter #20800:   Batch Loss = 1.746039, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.98149108887, Accuracy = 0.0606060624123\n",
      "Training iter #20900:   Batch Loss = 2.106161, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0955080986, Accuracy = 0.0606060624123\n",
      "Training iter #21000:   Batch Loss = 1.272783, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17817306519, Accuracy = 0.0606060624123\n",
      "Training iter #21100:   Batch Loss = 1.653381, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.23012256622, Accuracy = 0.0606060624123\n",
      "Training iter #21200:   Batch Loss = 1.645244, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18608427048, Accuracy = 0.0606060624123\n",
      "Training iter #21300:   Batch Loss = 1.738036, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.11801481247, Accuracy = 0.0303030312061\n",
      "Training iter #21400:   Batch Loss = 1.330666, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18190717697, Accuracy = 0.0454545468092\n",
      "Training iter #21500:   Batch Loss = 1.680324, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.2029504776, Accuracy = 0.0454545468092\n",
      "Training iter #21600:   Batch Loss = 1.563242, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20439338684, Accuracy = 0.0454545468092\n",
      "Training iter #21700:   Batch Loss = 1.728700, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20376110077, Accuracy = 0.0454545468092\n",
      "Training iter #21800:   Batch Loss = 1.199120, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.20436429977, Accuracy = 0.0454545468092\n",
      "Training iter #21900:   Batch Loss = 1.665675, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.19901371002, Accuracy = 0.0454545468092\n",
      "Training iter #22000:   Batch Loss = 1.326625, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18008518219, Accuracy = 0.0454545468092\n",
      "Training iter #22100:   Batch Loss = 1.689599, Accuracy = 0.650000035763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15749263763, Accuracy = 0.0454545468092\n",
      "Training iter #22200:   Batch Loss = 1.296417, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18007993698, Accuracy = 0.0303030312061\n",
      "Training iter #22300:   Batch Loss = 1.939761, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18843317032, Accuracy = 0.0454545468092\n",
      "Training iter #22400:   Batch Loss = 1.388144, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.1780295372, Accuracy = 0.0303030312061\n",
      "Training iter #22500:   Batch Loss = 1.548257, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.14904880524, Accuracy = 0.0303030312061\n",
      "Training iter #22600:   Batch Loss = 1.317715, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17392206192, Accuracy = 0.0454545468092\n",
      "Training iter #22700:   Batch Loss = 1.965459, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.17034339905, Accuracy = 0.0303030312061\n",
      "Training iter #22800:   Batch Loss = 1.391654, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15500354767, Accuracy = 0.0151515156031\n",
      "Training iter #22900:   Batch Loss = 1.398600, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.18549633026, Accuracy = 0.0151515156031\n",
      "Training iter #23000:   Batch Loss = 1.318394, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.16860580444, Accuracy = 0.0151515156031\n",
      "Training iter #23100:   Batch Loss = 1.925838, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.13328075409, Accuracy = 0.0151515156031\n",
      "Training iter #23200:   Batch Loss = 1.360971, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.04245471954, Accuracy = 0.0303030312061\n",
      "Training iter #23300:   Batch Loss = 1.407458, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.10114908218, Accuracy = 0.0303030312061\n",
      "Training iter #23400:   Batch Loss = 1.416122, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.21806430817, Accuracy = 0.0151515156031\n",
      "Training iter #23500:   Batch Loss = 1.961981, Accuracy = 0.700000047684\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.15421867371, Accuracy = 0.0303030312061\n",
      "Training iter #23600:   Batch Loss = 1.966697, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0921959877, Accuracy = 0.0303030312061\n",
      "Training iter #23700:   Batch Loss = 1.707716, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0912103653, Accuracy = 0.0151515156031\n",
      "Training iter #23800:   Batch Loss = 1.879596, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 5.0417470932, Accuracy = 0.0454545468092\n",
      "Training iter #23900:   Batch Loss = 1.933463, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 4.93729257584, Accuracy = 0.0606060624123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba25bbe38a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         feed_dict={\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         }\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" iterations at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs = extract_batch_size(sensor_dataset.x_train, step, batch_size)\n",
    "    batch_ys = extract_batch_size(sensor_dataset.y_train, step, batch_size)\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print \"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc)\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: sensor_dataset.x_test,\n",
    "                y: sensor_dataset.y_test\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print \"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print \"Optimization Finished!\"\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: sensor_dataset.x_test,\n",
    "        y: sensor_dataset.y_test\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print \"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
